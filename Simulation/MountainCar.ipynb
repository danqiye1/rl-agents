{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "MountainCar.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP3DHmIVP89YB115yicm9nz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/danqiye1/rl-agents/blob/master/Simulation/MountainCar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Setup Colab Environment\n",
        "!git clone git@github.com:danqiye1/rl-agents.git\n",
        "!pip install pipenv\n",
        "!pipenv install ."
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "\"\"\"\n",
        "Experiments for MountainCar\n",
        "\"\"\"\n",
        "import gym\n",
        "import telegram_send\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from matplotlib import pyplot as plt\n",
        "from MountainCar.agents import QLearningAgent, TDLambdaAgent\n",
        "from AgentUtils.plots import plot_running_avg"
      ],
      "outputs": [],
      "metadata": {
        "id": "lQK3_wYCStRP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "env = gym.make(\"MountainCar-v0\")\n",
        "agent = QLearningAgent(env, use_sklearn=True)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "N = 1000\n",
        "total_rewards = np.empty(N)\n",
        "for n in tqdm(range(N)):\n",
        "    # There are 3 different kinds of epsilon to try\n",
        "    #eps = 1.0/np.sqrt(n+1)\n",
        "    #eps = 0.3\n",
        "    eps = 0.1*(0.97**n)\n",
        "    total_reward = agent.play(epsilon=eps, env=env, gamma=0.99)\n",
        "    total_rewards[n] = total_reward\n",
        "    if (n + 1) % 100 == 0:\n",
        "        telegram_send.send(messages=[\"episode: {}, total reward: {}\".format(n, total_reward)])\n",
        "\n",
        "avgReward = total_rewards[-100:].mean()\n",
        "totalsteps = -total_rewards.sum()\n",
        "telegram_send.send(messages=[\n",
        "    \"Agent training complete! Please check your plots.\",\n",
        "    \"Average reward for last 100 episodes: {}.\".format(avgReward),\n",
        "    \"Total steps: {}\".format(totalsteps)\n",
        "])"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Sanity check on training\n",
        "plt.plot(total_rewards)\n",
        "plt.title(\"Rewards\")\n",
        "plt.show()"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "plot_running_avg(total_rewards)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Test if the agent is properly trained\n",
        "agent.play(epsilon=0, env=env, gamma=0.99, render=True)"
      ],
      "outputs": [],
      "metadata": {}
    }
  ]
}